High.test <- High[-train]
tree.carseats <- tree(High ~ . -Sales, Carseats, subset = train)
tree.pred <- predict(tree.carseats, Carseats, type = 'class')
table(tree.pred, High.test)
# cross-validation
# to determine the optimal level of tree complexity
set.seed(3)
(cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass))
names(cv.carseats)
# size: the number of terminal nodes of each tree considered
# dev : error rate
# k   : the value of the cost-complexity parameter used
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = 'b')
plot(cv.carseats$k, cv.carseats$dev, type = 'b')
# prune the tree to obtain the 9-node tree
prune.carseats <- prune.misclass(tree.carseats)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.pred <- predict(prune.carseats, Carseats.test, type = 'class')
table(tree.pred, High.test)
##### 8.3.8 Fitting Regression Trees
plot(cv.carseats$size, cv.carseats$dev, type = 'b')
plot(cv.carseats$k, cv.carseats$dev, type = 'b')
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = 'b')
plot(cv.carseats$k, cv.carseats$dev, type = 'b')
library(MASS)
train <- sample(nrow(Boston), nrow(Boston)/2)
##### 8.3.8 Fitting Regression Trees
set.seed(1)
train <- sample(nrow(Boston), nrow(Boston)/2)
table(tree.pred, High.test)
tree.pred <- predict(prune.carseats, Carseats.test, type = 'class')
table(tree.pred, High.test)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
# prune the tree to obtain the 9-node tree
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
par(mfrow = c(1, 2))
# prune the tree to obtain the 9-node tree
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
plot(prune.carseats)
text(prune.carseats)
text(prune.carseats, pretty = 1)
plot(prune.carseats)
text(prune.carseats, pretty = 1)
text(prune.carseats, pretty = 0)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
plot(prune.carseats)
text(prune.carseats, pretty = 1)
table(tree.pred, High.test)
tree.pred <- predict(prune.carseats, Carseats.test, type = 'class')
table(tree.pred, High.test)
set.seed(2)
train <- sample(nrow(Carseats), 200)
Carseats.test <- Carseats[-train,]
High.test <- High[-train]
tree.carseats <- tree(High ~ . -Sales, Carseats, subset = train)
tree.pred <- predict(tree.carseats, Carseats, type = 'class')
table(tree.pred, High.test)
tree.pred <- predict(tree.carseats, Carseats, type = 'class')
table(tree.pred, High.test)
##### CH8 Lab: Decision Trees #####
##### 8.3.1 Fitting Classification Trees
attach(Carseats)
High <- ifelse(Sales <= 8, 'No', 'Yes') # make a binary variable
Carseats <- data.frame(Carseats, High)
summary(tree.carseats <- tree(High ~ . -Sales, Carseats))
##### CH8 Lab: Decision Trees #####
##### 8.3.1 Fitting Classification Trees
attach(Carseats)
High <- ifelse(Sales <= 8, 'No', 'Yes') # make a binary variable
Carseats <- data.frame(Carseats, High)
summary(tree.carseats <- tree(High ~ . -Sales, Carseats))
High <- ifelse(Sales <= 8, 'No', 'Yes') # make a binary variable
View(Carseats)
rm(list=ls())
High <- ifelse(Sales <= 8, 'No', 'Yes') # make a binary variable
Carseats <- data.frame(Carseats, High)
summary(tree.carseats <- tree(High ~ . -Sales, Carseats))
plot(tree.carseats)
text(tree.carseats, pretty = 0)
set.seed(2)
train <- sample(nrow(Carseats), 200)
Carseats.test <- Carseats[-train,]
High.test <- High[-train]
tree.carseats <- tree(High ~ . -Sales, Carseats, subset = train)
tree.pred <- predict(tree.carseats, Carseats, type = 'class')
table(tree.pred, High.test)
length(tree.pred)
tree.pred <- predict(tree.carseats, Carseats.test, type = 'class')
table(tree.pred, High.test)
set.seed(3)
(cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass))
set.seed(3)
(cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass))
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = 'b')
plot(cv.carseats$k, cv.carseats$dev, type = 'b')
# prune the tree to obtain the 9-node tree
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.pred <- predict(prune.carseats, Carseats.test, type = 'class')
table(tree.pred, High.test)
##### 8.3.8 Fitting Regression Trees
set.seed(1)
train <- sample(nrow(Boston), nrow(Boston)/2)
tree.boston =
rm(list=ls())
##### 8.3.8 Fitting Regression Trees
set.seed(1)
train <- sample(nrow(Boston), nrow(Boston)/2)
tree.boston <-
?cv.tree
tree.carseats <- tree(High ~ . -Sales, Carseats, subset = train)
tree.pred <- predict(tree.carseats, Carseats.test, type = 'class')
table(tree.pred, High.test)
High <- ifelse(Sales <= 8, 'No', 'Yes') # make a binary variable
Carseats <- data.frame(Carseats, High)
summary(tree.carseats <- tree(High ~ . -Sales, Carseats))
plot(tree.carseats)
text(tree.carseats, pretty = 0)
set.seed(2)
train <- sample(nrow(Carseats), 200)
Carseats.test <- Carseats[-train,]
High.test <- High[-train]
tree.carseats <- tree(High ~ . -Sales, Carseats, subset = train)
tree.pred <- predict(tree.carseats, Carseats.test, type = 'class')
table(tree.pred, High.test)
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = 'b')
plot(cv.carseats$k, cv.carseats$dev, type = 'b')
# prune the tree to obtain the 9-node tree
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.pred <- predict(prune.carseats, Carseats.test, type = 'class')
table(tree.pred, High.test)
##### 8.3.8 Fitting Regression Trees
set.seed(1)
train <- sample(nrow(Boston), nrow(Boston)/2)
?prune.misclass
train <- sample(nrow(Boston), nrow(Boston)/2)
##### 8.3.8 Fitting Regression Trees
set.seed(1)
train <- sample(nrow(Boston), nrow(Boston)/2)
tree.boston <- tree(medv ~ ., Boston, subset = train)
summary(tree.boston <- tree(medv ~ ., Boston, subset = train))
plot(tree.boston)
par(mfrow = c(1, 1))
plot(prune.carseats)
text(prune.carseats, pretty = 0)
plot(tree.boston)
text(tree.boston)
plot(tree.boston)
text(tree.boston, pretty = 0)
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, boston$dev, type = 'b')
plot(cv.boston$size, cv.boston$dev, type = 'b')
?cv.tree
plot(cv.boston$size, cv.boston$dev, type = 'b')
prune.boston <- prune.tree(tree.boston, best = 5)
plot(prune.boston)
text(prune.boston, pretty = 0)
# deviance = Sum of Squared Errors
plot(tree.boston)
text(tree.boston, pretty = 0)
plot(prune.boston)
text(prune.boston, pretty = 0)
plot(prune.boston)
text(prune.boston, pretty = 0)
yhat <- predict(tree.boston, Boston[-train,])
boston.test <- Boston[-train, 'medv']
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)**2)
mean((yhat - boston.test)^2)
library(randomForest)
bag.boston <- randomForest(medv ~ ., data = Boston, subset = train,
mtry = 13, importance = TRUE)
##### 8.3.3 Bagging and Random Forest
set.seed(1)
bag.boston <- randomForest(medv ~ ., data = Boston, subset = train,
mtry = 13, importance = TRUE)
(bag.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 13, importance = TRUE))
##### 8.3.3 Bagging and Random Forest
set.seed(1)
(bag.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 13, importance = TRUE))
dim(Boston)
# bagging에서는 mtry = p (전체 설명변수를 모두 split variable로 사용)
yhat.bag <- predict(bag.boston, Boston[-train,])
plot(yhat.bag, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2) # MSE
mean((yhat.bag - boston.test)^2)
(rf.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 6, importance = TRUE))
# Random Forest
set.seed(1)
(rf.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 6, importance = TRUE))
yhat.rf <- predict(rf.boston, Boston[-train,])
plot(yhat.rf, boston.test)
abline(0, 1)
mean((yhat.rf - boston.test)^2)
importance(rf.boston)
varImpPlot(rf.boston)
install.packages('gbm')
library(gbm)
boost.boston <- gbm(medv ~ ., data = Boston[-train,], distribution = 'gaussian',
ntrees = 5000, interaction.depth = 4)
##### 8.3.4 Boosting
set.seed(1)
summary(boost.boston <- gbm(
medv ~ ., data = Boston[-train,], distribution = 'gaussian', ntrees = 5000, interaction.depth = 4
))
##### 8.3.4 Boosting
set.seed(1)
summary(boost.boston <- gbm(
medv ~ ., data = Boston[-train,], distribution = 'gaussian', n.trees = 5000, interaction.depth = 4
))
plot(boost.boston, i = 'rm')
# partial dependence plots for some important variables
par(mfrow = c(1, 2))
plot(boost.boston, i = 'rm')
plot(boost.boston, i = 'lstat')
yhat.boost <- predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
summary(boost.boston)
boost.boston
yhat.boost <- predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
plot(yhat.boost, boston.test)
abline(0, 1)
mean((yhat.boost - boston.test)^2)
set.seed(1)
train <- sample(nrow(Boston), nrow(Boston)/2)
summary(tree.boston <- tree(medv ~ ., Boston, subset = train))
# deviance = Sum of Squared Errors
plot(tree.boston)
text(tree.boston, pretty = 0)
cv.boston <- cv.tree(tree.boston) # default FUN = prune.tree
plot(cv.boston$size, cv.boston$dev, type = 'b') # 가장 복잡한 모형이 선택됨
prune.boston <- prune.tree(tree.boston, best = 5) # 5-node tree
plot(prune.boston)
text(prune.boston, pretty = 0)
yhat <- predict(tree.boston, Boston[-train,])
boston.test <- Boston[-train, 'medv']
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2) # MSE
##### 8.3.3 Bagging and Random Forest
# Bagging
set.seed(1)
(bag.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 13, importance = TRUE))
# bagging에서는 mtry = p (전체 설명변수를 모두 split variable로 사용)
yhat.bag <- predict(bag.boston, Boston[-train,])
plot(yhat.bag, boston.test)
abline(0, 1)
mean((yhat.bag - boston.test)^2)
# Random Forest
set.seed(1)
(rf.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 6, importance = TRUE))
yhat.rf <- predict(rf.boston, Boston[-train,])
plot(yhat.rf, boston.test)
abline(0, 1)
mean((yhat.rf - boston.test)^2)
importance(rf.boston)
varImpPlot(rf.boston)
##### 8.3.4 Boosting
set.seed(1)
summary(boost.boston <- gbm(
medv ~ ., data = Boston[-train,], distribution = 'gaussian', n.trees = 5000, interaction.depth = 4
))
# partial dependence plots for some important variables
par(mfrow = c(1, 2))
plot(boost.boston, i = 'rm')    # rm이 높을수록 y가 높아짐
plot(boost.boston, i = 'lstat') # lstat이 높을수록 y가 작아짐
yhat.boost <- predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)
par(mfrow = c(1, 1))
plot(yhat.boost, boston.test)
abline(0, 1)
mean((yhat.boost - boston.test)^2)
boston.test
yhat.boost
summary(boost.boston <- gbm(
medv ~ ., data = Boston[train,], distribution = 'gaussian', n.trees = 5000, interaction.depth = 4
))
##### 8.3.4 Boosting
set.seed(1)
summary(boost.boston <- gbm(
medv ~ ., data = Boston[train,], distribution = 'gaussian', n.trees = 5000, interaction.depth = 4
))
# partial dependence plots for some important variables
par(mfrow = c(1, 2))
plot(boost.boston, i = 'rm')    # rm이 높을수록 y가 높아짐
plot(boost.boston, i = 'lstat') # lstat이 높을수록 y가 작아짐
yhat.boost <- predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)
par(mfrow = c(1, 1))
plot(boost.boston, i = 'rm')    # rm이 높을수록 y가 높아짐
plot(boost.boston, i = 'lstat') # lstat이 높을수록 y가 작아짐
yhat.boost <- predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)
par(mfrow = c(1, 1))
plot(yhat.boost, boston.test)
abline(0, 1)
mean((yhat.boost - boston.test)^2)
boost.boston
?gbm
#####
install.packages('xgboost')
library(xgboost)
library(data.table)
library(mlr)
# set variable names
setcol <- c(
'age',` `'workclass',` `'fnlwgt',` `'education'
)
# set variable names
setcol <- c(
'age',` `'workclass',` `'fnlwgt',` `'education'
)
setcol <- c("age",` `"workclass",` `"fnlwgt",` `"education",` `"education-num",` `"marital-status",` `"occupation",` `"relationship",` `"race",` `"sex",` `"capital-gain",` `"capital-loss",` `"hours-per-week",` `"native-country",` `"target")
# set variable names
setcol <- c('age', 'workclass', 'fnlwgt', 'education', 'education-num',
'marital-status', 'occupation', 'relationship', 'race', 'sex',
'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'target')
# load data
train <- read.table('adult.data', header = F, sep = ',', col.names = setcol,
na.strings = c('?'), stringsAsFactors = F)
# load data
train <- read.table('adult.data', header = F, sep = ',', col.names = setcol,
na.strings = c('?'), stringsAsFactors = F)
# load data
train <- read.table('adult.data.DATA', header = F, sep = ',', col.names = setcol,
na.strings = c('?'), stringsAsFactors = F)
# load data
train <- read.table('adultdata.txt', header = F, sep = ',', col.names = setcol,
na.strings = c('?'), stringsAsFactors = F)
setwd('C:/Users/HK/Desktop/GitHub/Graduate/DataMining/XGBoost')
# load data
train <- read.table('adultdata.txt', header = F, sep = ',', col.names = setcol,
na.strings = c('?'), stringsAsFactors = F)
# load data
train <- read.table('adultdata.txt', header = F, sep = ',', col.names = setcol,
na.strings = c('?'), stringsAsFactors = F)
test <- read.table('adult.TEST', header = F, sep = ',', col.names = setcol,
na.strings = c('?'), stringsAsFactors = F)
test <- read.table('adulttest.txt', header = F, sep = ',', col.names = setcol,
na.strings = c('?'), stringsAsFactors = F)
test <- read.table('adulttest.txt', header = F, sep = ',', col.names = setcol,
na.strings = c('?'), stringsAsFactors = F)
test <- read.table('adult.TEST', header = F, sep = ',', col.names = setcol, skip = 1,
na.strings = c('?'), stringsAsFactors = F)
# load data
train <- read.table('adult.DATA', header = F, sep = ',', col.names = setcol,
na.strings = c('?'), stringsAsFactors = F)
# load data
train <- read.table('adult.DATA', header = F, sep = ',', col.names = setcol,
na.strings = c('?'), stringsAsFactors = F)
test <- read.table('adult.TEST', header = F, sep = ',', col.names = setcol, skip = 1,
na.strings = c('?'), stringsAsFactors = F)
# convert data.frame to data.table
setDT(train)
setDT(test)
# check missing values
table(is.na(train))
sapply(train, function(x) sum(is.na(x)))
sapply(train, function(x) sum(is.na(x))/length(x))
apply(train, function(x) sum(is.na(x))/length(x))
apply(train, 2,function(x) sum(is.na(x))/length(x))
?sapply
table(is.na(test))
sapply(test, function(x) sum(is.na(x))/length(x))
str(test)
# quick data cleaning
# remove extra character from target variable
library(stringr)
target
2 := 1
test[,test$target := substr(test$target, 1, nchar(test$target)-1)]
test[,test$target != substr(test$target, 1, nchar(test$target)-1)]
substr(test$target, 1, nchar(test$target)-1)
test [,test$target != substr(test$target, 1, nchar(test$target)-1)]
test [,test$target := substr(test$target, 1, nchar(test$target)-1)]
test [,target := substr(target, 1, nchar(target)-1)]
test
install.packages("mlr")
sapply(test, is.character)
# remove leading whitespaces
char_col <- colnames(train)[sapply(test, is.character)]
char_col
for (i in char_col) set(train, j = i, value = str_trim(train[[i]]), side = 'left')
for (i in char_col) set(train, j = i, value = str_trim(train[[i]], side = 'left'))
for (i in char_col) set(test, j = i, value = str_trim(train[[i]], side = 'left'))
for (i in char_col) set(test, j = i, value = str_trim(test[[i]], side = 'left'))
# set all missing value as 'Missing'
train[is.na(train)] <- 'Missing'
test[is.na(test)] <- 'Missing'
test[is.na(test)]
# using one hot encoding
label <- train$target
label_test <- test$target
new_tr <- model.matrix(~ . + 0, data = train[,-c('target'), with = F])
new_ts <- model.matrix(~ . + 0, data = test[,-c('target'), with = F])
new_tr
# convert factor to numeric
label <- as.numeric(label) - 1
label_test <- as.numeric(label_test) - 1
label
# using one hot encoding
label <- train$target
label_test <- test$target
label
# 1.2 Installation
library(xgboost)
# 1.3 Learning
# 1.3.2 Dataset loading
# use Mushroom data
data(agaricus.train, package = 'xgboost')
data(agaricus.test, package = 'xgboost')
train <- agaricus.train
test <- agaricus.test
str(train) # data (X ; dgCMatrix class), label (y)
dim(train$data) # 80%
dim(test$data)  # 20%
# dgCMatrix class
bstSparse <- xgboost(
data = train$data, label = train$label,
max_depth = 2, eta = 1, nthread = 2, nrounds = 2,
objective = 'binary:logistic'
)
table(train$label)
setwd('C:/Users/HK/Desktop/GitHub/Graduate/DataMining/XGBoost')
##### XGBoost: Parameter Tuning #####
library(data.table)
library(mlr)
# set variable names
setcol <- c('age', 'workclass', 'fnlwgt', 'education', 'education-num',
'marital-status', 'occupation', 'relationship', 'race', 'sex',
'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'target')
# load data
train <- read.table('adult.DATA', header = F, sep = ',', col.names = setcol,
na.strings = c('?'), stringsAsFactors = F)
test <- read.table('adult.TEST', header = F, sep = ',', col.names = setcol, skip = 1,
na.strings = c('?'), stringsAsFactors = F)
# convert data.frame to data.table
setDT(train)
setDT(test)
?data.table
# check missing values
table(is.na(train))
sapply(train, function(x) sum(is.na(x))/length(x))
table(is.na(test))
sapply(test, function(x) sum(is.na(x))/length(x))
# quick data cleaning
# remove extra character from target variable
library(stringr)
test [,target := substr(target, 1, nchar(target)-1)] # (?)
# remove leading whitespaces
char_col <- colnames(train)[sapply(test, is.character)]
for (i in char_col) set(train, j = i, value = str_trim(train[[i]], side = 'left'))
for (i in char_col) set(test, j = i, value = str_trim(test[[i]], side = 'left'))
# set all missing value as 'Missing'
train[is.na(train)] <- 'Missing'
test[is.na(test)] <- 'Missing'
# using one hot encoding
label <- train$target
label_test <- test$target
new_tr <- model.matrix(~ . + 0, data = train[,-c('target'), with = F])
new_ts <- model.matrix(~ . + 0, data = test[,-c('target'), with = F])
# convert factor to numeric
label <- as.numeric(label) - 1  # ?????
label_test <- as.numeric(label_test) - 1 # ?????
# using one hot encoding
label <- train$target
laberl
label
as.numeric
as.numeric(label)
test <- read.table('adult.TEST', header = F, sep = ',', col.names = setcol, skip = 1,
na.strings = c('?'), stringsAsFactors = F)
setDT(test)
table(is.na(test))
sapply(test, function(x) sum(is.na(x))/length(x))
test$target
test [,target := substr(target, 1, nchar(target)-1)] # (?)
test
test$target
# remove leading whitespaces
char_col <- colnames(train)[sapply(test, is.character)]
cahr_col
char_col
?set
# remove leading whitespaces
char_col <- colnames(train)[sapply(test, is.character)]
for (i in char_col) set(train, j = i, value = str_trim(train[[i]], side = 'left'))
for (i in char_col) set(test, j = i, value = str_trim(test[[i]], side = 'left'))
train[is.na(train)]
# set all missing value as 'Missing'
train[is.na(train)] <- 'Missing'
test[is.na(test)] <- 'Missing'
# set all missing value as 'Missing'
train[is.na(train)] <- 'Missing'
train[is.na(train)]
is.na(train)

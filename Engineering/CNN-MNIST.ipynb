{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN를 이용한 MNIST 자료 분석\n",
    "\n",
    "  \n",
    "과목명 | 금융공학  \n",
    "교수명 | 안재윤 교수님  \n",
    "제출일 | 2018년 11월 20일  \n",
    "  \n",
    "학　번 | 182STG18  \n",
    "이　름 | 이하경  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 필요한 패키지 및 함수, MNIST 데이터 불러오기\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array([np.reshape(x, (28, 28, 1)) for x in mnist.train.images])\n",
    "test_x = np.array([np.reshape(x, (28, 28, 1)) for x in mnist.test.images])\n",
    "train_y = mnist.train.labels\n",
    "test_y =  mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 28, 1) (10000, 28, 28, 1) (55000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Convolution Layer에 사용하도록 image 데이터의 차원을 4차원으로 변환하여 저장 \n",
    "# (Nobs, pixel의 H, pixel의 W, input Channel)\n",
    "\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "t = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# 학습 단계마다 batch size 만큼의 x와 y 데이터를 feed시켜줄 placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Convolution Layer 1] Convolution → Relu → Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 단계마다 값을 업데이트할 Variable \n",
    "# (w의 초기값은 truncated normal random number, bias의 초기값은 0.1 (약간의 양수로 초기화))\n",
    "\n",
    "w_conv1 = tf.Variable(tf.truncated_normal(shape = [4, 4, 1, 32], stddev = 0.1)) # [FW1, FH1, C, FN1]\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape = [32]))\n",
    "\n",
    "# 4*4 필터 설정, Input data의 채널 수 1, 필터의 채널을 32개로 확장하여 output\n",
    "\n",
    "h_conv1 = tf.nn.conv2d(x, w_conv1, strides = [1, 1, 1, 1], padding = 'SAME') \n",
    "h_relu1 = tf.nn.relu(h_conv1 + b_conv1)\n",
    "\n",
    "# 필터를 한칸씩 이동, padding size는 input과 output size가 동일하도록 자동설정함\n",
    "# x와 weights의 합성곱과 bias을 더한 값을 활성함수 relu에 입력\n",
    "\n",
    "h_pool1 = tf.nn.max_pool(h_relu1, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "\n",
    "# pooling 단계에서 2*2 필터를 2칸씩 이동하며 필터마다 가장 큰 값을 저장하여 \n",
    "# 첫번째 pooling을 거친 후 차원은 [batch size, 14, 14, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Convolutional Layer 2] Convolution → Relu→ Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_conv2 = tf.Variable(tf.truncated_normal(shape = [4, 4, 32, 64], stddev = 0.1))\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape = [64]))\n",
    "\n",
    "# Conv. Layer 1과 동일하게 4*4 필터 설정, Layer 2의 Input 채널의 수는 32로 64의 채널로 확장하여 output\n",
    "\n",
    "h_conv2 = tf.nn.conv2d(h_pool1, w_conv2, strides = [1, 1, 1, 1], padding = 'SAME') \n",
    "h_relu2 = tf.nn.relu(h_conv2 + b_conv2)\n",
    "h_pool2 = tf.nn.max_pool(h_relu2, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME') \n",
    "\n",
    "# 두번째 pooling을 거친 후의 차원은 [batch size, 7, 7, 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래부터의 과정은 기존의 DNN와 동일하다.\n",
    "#### [Full-Connected Layer 1] Affine → Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "# Affine layer에 입력하기 위해 2차원으로 변경 (-1은 기존의 mini-batch 차원을 유지해준다)\n",
    "\n",
    "w_fc1 = tf.Variable(tf.truncated_normal(shape = [7*7*64, 128], stddev = 0.1))\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape = [128]))\n",
    "\n",
    "# node의 개수를 128로 하여 7*7*64개의 특성이 128개의 node와 모두 연결되도록 한다\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Full-Connected Layer 2] Affine → Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affine Layer을 한 층 더 추가하였다 (node의 개수는 128개로 동일하게 설정)\n",
    "\n",
    "w_fc2 = tf.Variable(tf.truncated_normal(shape = [128, 128], stddev = 0.1))\n",
    "b_fc2 = tf.Variable(tf.constant(0.1, shape = [128]))\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1, w_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Output Layer] Affine → Softmax (*Final Output!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_fc3 = tf.Variable(tf.truncated_normal(shape = [128, 10], stddev = 0.1))              \n",
    "b_fc3 = tf.Variable(tf.constant(0.1, shape = [10]))\n",
    "p = tf.nn.softmax(tf.matmul(h_fc2, w_fc3) + b_fc3)                           \n",
    "\n",
    "# 최종적으로 관측치당 길이 10의 벡터로 출력됨 (관측치 당 10개의 클래스 중 하나로 분류될 확률 값 출력)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 단계와 평가 단계에 필요한 함수들을 지정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(- tf.reduce_sum(t*tf.log(p), 1))   # 미니배치마다의 -y*log(p)의 합의 평균, 작을 수록 좋음\n",
    "train_step = tf.train.AdamOptimizer(0.01).minimize(cross_entropy) # Adam Optimizer의 경사하강률 0.01을 이용하여 손실함수를 최소화함\n",
    "correct_pred = tf.equal(tf.argmax(p, 1), tf.argmax(t, 1))         # 분류될 class와 실제 class 값이 같으면 1, 같지 않으면 0\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))      # 전체 데이터 중 몇 개를 정확히 맞추는지 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습의 모든 준비가 되었으므로 session을 running하기 전 variable들을 initialize시킴\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 10 \n",
    "n_batch = train_x.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    x_, y_ = shuffle(train_x, train_y)\n",
    "    \n",
    "    for i in range(n_batch):\n",
    "        start = i*batch_size\n",
    "        end = start + batch_size\n",
    "\n",
    "        sess.run(train_step, feed_dict = { x:x_[start:end], t:y_[start:end] })\n",
    "        cost = sess.run(cross_entropy, feed_dict = { x:x_[start:end], t:y_[start:end] })\n",
    "\n",
    "#        if i % 10 == 0:\n",
    "#            print(cost)\n",
    "            \n",
    "# 총 10번의 학습 단계 동안 512개의 mini batch를 55000 // 512 = 107번 수행해준다\n",
    "# shuffle 함수를 이용해 train의 x와 y 데이터의 순서를 무작위로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9719"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.eval(session = sess, feed_dict = { x:test_x, t:test_y })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9737091\n",
      "0.97374547\n"
     ]
    }
   ],
   "source": [
    "print(accuracy.eval(session = sess, feed_dict = { x:train_x[:27500], t:train_y[:27500] }))\n",
    "print(accuracy.eval(session = sess, feed_dict = { x:train_x[27500:], t:train_y[27500:] })) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
